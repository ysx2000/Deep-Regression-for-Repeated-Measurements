{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0, training loss = 0.01636433179697229, validation loss = 0.009717670269310474\n",
      "epoch = 1, training loss = 0.009351519195155965, validation loss = 0.009196714498102665\n",
      "epoch = 2, training loss = 0.009308549808338284, validation loss = 0.009184739552438259\n",
      "epoch = 3, training loss = 0.009334208288540443, validation loss = 0.009176759049296379\n",
      "epoch = 4, training loss = 0.00925175874080095, validation loss = 0.00917289312928915\n",
      "epoch = 5, training loss = 0.009233237295928929, validation loss = 0.009172727353870869\n",
      "epoch = 6, training loss = 0.009257238151298629, validation loss = 0.00916895642876625\n",
      "epoch = 7, training loss = 0.009294960104549924, validation loss = 0.009168082848191261\n",
      "epoch = 8, training loss = 0.00927514655308591, validation loss = 0.009175512008368969\n",
      "epoch = 9, training loss = 0.009259058395400643, validation loss = 0.009175993502140045\n",
      "epoch = 10, training loss = 0.009270785686870417, validation loss = 0.009165740571916103\n",
      "epoch = 11, training loss = 0.009206904576987855, validation loss = 0.009168009273707867\n",
      "epoch = 12, training loss = 0.009298455228822099, validation loss = 0.009245623834431171\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 13, training loss = 0.009286622672031323, validation loss = 0.009169278666377068\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 14, training loss = 0.009259749721321795, validation loss = 0.009167534299194813\n",
      "epoch = 15, training loss = 0.009277291414845321, validation loss = 0.009176244959235191\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 16, training loss = 0.009280821774154902, validation loss = 0.009180868975818157\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 17, training loss = 0.009283778697459234, validation loss = 0.00918457843363285\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 18, training loss = 0.009280931385647919, validation loss = 0.009164883755147457\n",
      "epoch = 19, training loss = 0.009254961533264982, validation loss = 0.009165232069790363\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 20, training loss = 0.009298054501414299, validation loss = 0.009171566925942898\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 21, training loss = 0.009238288893053928, validation loss = 0.009207544848322868\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 22, training loss = 0.009245765783513585, validation loss = 0.009229052811861038\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 23, training loss = 0.009331655481623279, validation loss = 0.009206785820424557\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 24, training loss = 0.009261217459829317, validation loss = 0.009182784706354141\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 25, training loss = 0.009259664945097433, validation loss = 0.009175306186079979\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 26, training loss = 0.009284576064803533, validation loss = 0.009210417047142982\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 27, training loss = 0.009286872292351391, validation loss = 0.009181782603263855\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 28, training loss = 0.009303061627886362, validation loss = 0.009164894931018353\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "epoch = 0, training loss = 0.04393595056090918, validation loss = 0.010376649908721447\n",
      "epoch = 1, training loss = 0.009705915296864178, validation loss = 0.009404261596500874\n",
      "epoch = 2, training loss = 0.0093991511190931, validation loss = 0.009283570572733879\n",
      "epoch = 3, training loss = 0.009326959054710137, validation loss = 0.009245148859918118\n",
      "epoch = 4, training loss = 0.009377971808943484, validation loss = 0.009237275458872318\n",
      "epoch = 5, training loss = 0.009327428699988458, validation loss = 0.0092318719252944\n",
      "epoch = 6, training loss = 0.009337568500389656, validation loss = 0.009214143268764019\n",
      "epoch = 7, training loss = 0.009305879603036575, validation loss = 0.009192712604999542\n",
      "epoch = 8, training loss = 0.00922649617617329, validation loss = 0.00918262917548418\n",
      "epoch = 9, training loss = 0.009259227456318008, validation loss = 0.009177260100841522\n",
      "epoch = 10, training loss = 0.009257804524774352, validation loss = 0.009173193946480751\n",
      "epoch = 11, training loss = 0.009249552256531186, validation loss = 0.009172191843390465\n",
      "epoch = 12, training loss = 0.009285820208282934, validation loss = 0.009171705693006516\n",
      "epoch = 13, training loss = 0.009210674868275722, validation loss = 0.009177601896226406\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 14, training loss = 0.009341158024552796, validation loss = 0.0091699892655015\n",
      "epoch = 15, training loss = 0.009216890916124813, validation loss = 0.009171037934720516\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 16, training loss = 0.009309496213164594, validation loss = 0.009174060076475143\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 17, training loss = 0.009250362714131674, validation loss = 0.009168707765638828\n",
      "epoch = 18, training loss = 0.009270807521210777, validation loss = 0.009168961085379124\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 19, training loss = 0.009219443697171906, validation loss = 0.009168528951704502\n",
      "epoch = 20, training loss = 0.009251405976505743, validation loss = 0.009168007411062717\n",
      "epoch = 21, training loss = 0.009284305153414607, validation loss = 0.009169812314212322\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 22, training loss = 0.009248849392558137, validation loss = 0.00916848424822092\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 23, training loss = 0.00921437622875803, validation loss = 0.009167974814772606\n",
      "epoch = 24, training loss = 0.009295896109607484, validation loss = 0.009171441197395325\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 25, training loss = 0.00924120998630921, validation loss = 0.00917133130133152\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 26, training loss = 0.009310572589230206, validation loss = 0.009174119681119919\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 27, training loss = 0.009311944711953402, validation loss = 0.009166383184492588\n",
      "epoch = 28, training loss = 0.009272679996987184, validation loss = 0.009166098199784756\n",
      "epoch = 29, training loss = 0.009232397242966626, validation loss = 0.00916931964457035\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 30, training loss = 0.009269557453484999, validation loss = 0.009167706593871117\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 31, training loss = 0.009272911870438192, validation loss = 0.00916686188429594\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 32, training loss = 0.009306431256441606, validation loss = 0.009166490286588669\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 33, training loss = 0.009268671843326755, validation loss = 0.009165860712528229\n",
      "epoch = 34, training loss = 0.009246702616413435, validation loss = 0.009171121753752232\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 35, training loss = 0.009276133987845646, validation loss = 0.009169081225991249\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 36, training loss = 0.009266455787130527, validation loss = 0.009168693795800209\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 37, training loss = 0.009302201913669705, validation loss = 0.009172117337584496\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 38, training loss = 0.009255257745583853, validation loss = 0.009173265658318996\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 39, training loss = 0.00924804948994683, validation loss = 0.00916643999516964\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 40, training loss = 0.00923793284325964, validation loss = 0.009169504977762699\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 41, training loss = 0.009336690832343366, validation loss = 0.009176266379654408\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 42, training loss = 0.009314742534317903, validation loss = 0.009170172736048698\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 43, training loss = 0.009245287989162736, validation loss = 0.00916824210435152\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "epoch = 0, training loss = 0.111277129733935, validation loss = 0.009206656366586685\n",
      "epoch = 1, training loss = 0.009301098839690288, validation loss = 0.009225437417626381\n",
      "epoch = 2, training loss = 0.009302955146671997, validation loss = 0.009188469499349594\n",
      "epoch = 3, training loss = 0.009247196747714447, validation loss = 0.009213538840413094\n",
      "epoch = 4, training loss = 0.009298829749847451, validation loss = 0.009184818714857101\n",
      "epoch = 5, training loss = 0.009303217029405965, validation loss = 0.00933469831943512\n",
      "epoch = 6, training loss = 0.00932928454130888, validation loss = 0.009347202256321907\n",
      "epoch = 7, training loss = 0.00929104906713797, validation loss = 0.009165922179818153\n",
      "epoch = 8, training loss = 0.009242399336977137, validation loss = 0.009180350229144096\n",
      "epoch = 9, training loss = 0.009249899924422303, validation loss = 0.009213223122060299\n",
      "epoch = 10, training loss = 0.00930603265037967, validation loss = 0.009178657084703445\n",
      "epoch = 11, training loss = 0.009215619932446215, validation loss = 0.009186994284391403\n",
      "epoch = 12, training loss = 0.009316428383398388, validation loss = 0.009171588346362114\n",
      "epoch = 13, training loss = 0.009321587729371257, validation loss = 0.009201554581522942\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 14, training loss = 0.00928388569607503, validation loss = 0.009163323789834976\n",
      "epoch = 15, training loss = 0.009275750489905477, validation loss = 0.009162935428321362\n",
      "epoch = 16, training loss = 0.009278823181779848, validation loss = 0.009169107303023338\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 17, training loss = 0.009235049817814596, validation loss = 0.009314145892858505\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 18, training loss = 0.00925065390765667, validation loss = 0.009171887300908566\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 19, training loss = 0.009237611511101326, validation loss = 0.00923320185393095\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 20, training loss = 0.009252215502783656, validation loss = 0.009175584651529789\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 21, training loss = 0.009216220325065983, validation loss = 0.009217560291290283\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 22, training loss = 0.009234711036293043, validation loss = 0.009179201908409595\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 23, training loss = 0.009251673343694873, validation loss = 0.009194784797728062\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 24, training loss = 0.009276299892614285, validation loss = 0.009231342002749443\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 25, training loss = 0.009275982751407556, validation loss = 0.009180097840726376\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "epoch = 0, training loss = 3.952627289859164, validation loss = 0.009194137528538704\n",
      "epoch = 1, training loss = 0.009261580235842202, validation loss = 0.009190634824335575\n",
      "epoch = 2, training loss = 0.009269594344206981, validation loss = 0.009170438162982464\n",
      "epoch = 3, training loss = 0.009230835505554246, validation loss = 0.009220772422850132\n",
      "epoch = 4, training loss = 0.009243196781931652, validation loss = 0.009170681238174438\n",
      "epoch = 5, training loss = 0.00929843463624517, validation loss = 0.00917286891490221\n",
      "epoch = 6, training loss = 0.009277909683684507, validation loss = 0.00918601080775261\n",
      "epoch = 7, training loss = 0.009265476372092962, validation loss = 0.00928686372935772\n",
      "epoch = 8, training loss = 0.009263076948829822, validation loss = 0.009183554910123348\n",
      "epoch = 9, training loss = 0.009229836856118508, validation loss = 0.00918671116232872\n",
      "epoch = 10, training loss = 0.009260854606206218, validation loss = 0.009172326885163784\n",
      "epoch = 11, training loss = 0.009317382807946868, validation loss = 0.009226059541106224\n",
      "epoch = 12, training loss = 0.009236468429056307, validation loss = 0.009179575368762016\n",
      "epoch = 13, training loss = 0.009246767395072512, validation loss = 0.009177446365356445\n",
      "epoch = 14, training loss = 0.009202846287128827, validation loss = 0.009228031150996685\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 15, training loss = 0.009236852871254086, validation loss = 0.009189808741211891\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 16, training loss = 0.009253190519909063, validation loss = 0.009175637736916542\n",
      "epoch = 17, training loss = 0.009251674119797017, validation loss = 0.00916651263833046\n",
      "epoch = 18, training loss = 0.009272565573660864, validation loss = 0.009170820005238056\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 19, training loss = 0.00927616404886875, validation loss = 0.009355989284813404\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 20, training loss = 0.00930548508444594, validation loss = 0.009167321026325226\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 21, training loss = 0.009243584212122692, validation loss = 0.009184683673083782\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 22, training loss = 0.009263250381789274, validation loss = 0.009165733121335506\n",
      "epoch = 23, training loss = 0.009249000266815225, validation loss = 0.009173297323286533\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 24, training loss = 0.009246752442171177, validation loss = 0.009199727326631546\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 25, training loss = 0.009293073762415184, validation loss = 0.009166236035525799\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 26, training loss = 0.009288817178457975, validation loss = 0.009198523126542568\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 27, training loss = 0.009283068589866161, validation loss = 0.009186594747006893\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 28, training loss = 0.009253961784351204, validation loss = 0.00918322429060936\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 29, training loss = 0.009224771884166531, validation loss = 0.009160946123301983\n",
      "epoch = 30, training loss = 0.009262914303690195, validation loss = 0.009169341996312141\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 31, training loss = 0.009304013931088977, validation loss = 0.009190091863274574\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 32, training loss = 0.009240171354677942, validation loss = 0.009170443750917912\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 33, training loss = 0.009225502765427033, validation loss = 0.009249611757695675\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 34, training loss = 0.009274076954979036, validation loss = 0.009163287468254566\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 35, training loss = 0.00925164227373898, validation loss = 0.009173081256449223\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 36, training loss = 0.009293348192133837, validation loss = 0.009168857708573341\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 37, training loss = 0.009218483076741299, validation loss = 0.009227262809872627\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 38, training loss = 0.00927585611740748, validation loss = 0.009162037633359432\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 39, training loss = 0.009285200904640887, validation loss = 0.009171570651233196\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "epoch = 0, training loss = 247.97211188367672, validation loss = 0.009597362950444221\n",
      "epoch = 1, training loss = 0.009307558260237178, validation loss = 0.009191262535750866\n",
      "epoch = 2, training loss = 0.00926090101711452, validation loss = 0.009205015376210213\n",
      "epoch = 3, training loss = 0.00929085672315624, validation loss = 0.009168281219899654\n",
      "epoch = 4, training loss = 0.009265018083776036, validation loss = 0.009197604842483997\n",
      "epoch = 5, training loss = 0.009263112261477444, validation loss = 0.009194070473313332\n",
      "epoch = 6, training loss = 0.009271434792834852, validation loss = 0.009205847047269344\n",
      "epoch = 7, training loss = 0.009308700268674228, validation loss = 0.009166676551103592\n",
      "epoch = 8, training loss = 0.009287036024034023, validation loss = 0.009168988093733788\n",
      "epoch = 9, training loss = 0.009261416581769785, validation loss = 0.009198023937642574\n",
      "epoch = 10, training loss = 0.009301032573502097, validation loss = 0.009196776896715164\n",
      "epoch = 11, training loss = 0.009292231613977088, validation loss = 0.009165518917143345\n",
      "epoch = 12, training loss = 0.00935961132765644, validation loss = 0.009465628303587437\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 13, training loss = 0.00933107469851772, validation loss = 0.009181061759591103\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 14, training loss = 0.009245001840301685, validation loss = 0.00921114906668663\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 15, training loss = 0.0093549067258007, validation loss = 0.00923445075750351\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 16, training loss = 0.009310672447706262, validation loss = 0.00917088333517313\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 17, training loss = 0.00930255752367278, validation loss = 0.009220483712852001\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 18, training loss = 0.00925228307541046, validation loss = 0.009196144528687\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 19, training loss = 0.009260626613265939, validation loss = 0.009186037816107273\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 20, training loss = 0.009255982469767332, validation loss = 0.009185237810015678\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 21, training loss = 0.009262519707489345, validation loss = 0.009169884026050568\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n",
      "epoch = 0, training loss = 1322.7616102300688, validation loss = 0.009971337392926216\n",
      "epoch = 1, training loss = 0.009349465551268723, validation loss = 0.009171800687909126\n",
      "epoch = 2, training loss = 0.00924438966386434, validation loss = 0.009174728766083717\n",
      "epoch = 3, training loss = 0.00925115250154502, validation loss = 0.00918061938136816\n",
      "epoch = 4, training loss = 0.009247025914697183, validation loss = 0.00917095597833395\n",
      "epoch = 5, training loss = 0.009282879065722227, validation loss = 0.009163078851997852\n",
      "epoch = 6, training loss = 0.009269821224734187, validation loss = 0.009164257906377316\n",
      "epoch = 7, training loss = 0.011580778259990944, validation loss = 0.010264581069350243\n",
      "epoch = 8, training loss = 0.010968625312671065, validation loss = 0.009702104143798351\n",
      "epoch = 9, training loss = 0.009649158584781818, validation loss = 0.009262515231966972\n",
      "epoch = 10, training loss = 0.009368793728450934, validation loss = 0.00926076341420412\n",
      "epoch = 11, training loss = 0.009315978192413846, validation loss = 0.009188045747578144\n",
      "epoch = 12, training loss = 0.00930031055274109, validation loss = 0.009234198369085789\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 13, training loss = 0.009294940313945213, validation loss = 0.009170901961624622\n",
      "epoch = 14, training loss = 0.009311818336654041, validation loss = 0.009198274463415146\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 15, training loss = 0.009261549424587024, validation loss = 0.009169952012598515\n",
      "epoch = 16, training loss = 0.00924639698738853, validation loss = 0.0091785229742527\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 17, training loss = 0.009330553183746006, validation loss = 0.009168329648673534\n",
      "epoch = 18, training loss = 0.009266613827397427, validation loss = 0.009164820425212383\n",
      "epoch = 19, training loss = 0.009284822865285806, validation loss = 0.009177024476230145\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 20, training loss = 0.009286732516354985, validation loss = 0.009169130586087704\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 21, training loss = 0.009289502502522536, validation loss = 0.009167207404971123\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 22, training loss = 0.00927693433024817, validation loss = 0.009164712391793728\n",
      "epoch = 23, training loss = 0.00928172024173869, validation loss = 0.009164517745375633\n",
      "epoch = 24, training loss = 0.009216050242280794, validation loss = 0.009184565395116806\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 25, training loss = 0.00922792866670837, validation loss = 0.009164033457636833\n",
      "epoch = 26, training loss = 0.009242852528889975, validation loss = 0.00916469469666481\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 27, training loss = 0.00924226807223426, validation loss = 0.009164503775537014\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 28, training loss = 0.009239497879106138, validation loss = 0.009163210168480873\n",
      "epoch = 29, training loss = 0.009310244168672297, validation loss = 0.009222079068422318\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 30, training loss = 0.009319500169820256, validation loss = 0.009247501380741596\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 31, training loss = 0.009267897707306676, validation loss = 0.009173181839287281\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 32, training loss = 0.009238099213689566, validation loss = 0.009166358970105648\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 33, training loss = 0.00930944599935578, validation loss = 0.009194029495120049\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 34, training loss = 0.009258428691989846, validation loss = 0.009165532886981964\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 35, training loss = 0.00929121838675605, validation loss = 0.009176303632557392\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 36, training loss = 0.009256347444736295, validation loss = 0.009162536822259426\n",
      "epoch = 37, training loss = 0.009264764143154025, validation loss = 0.009170610457658768\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 38, training loss = 0.009276331842152609, validation loss = 0.009185198694467545\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 39, training loss = 0.009228530831428038, validation loss = 0.009176652878522873\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 40, training loss = 0.009262331994250417, validation loss = 0.009161698631942272\n",
      "epoch = 41, training loss = 0.00927283715767165, validation loss = 0.009172490797936916\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 42, training loss = 0.009281250699940655, validation loss = 0.00918648298829794\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 43, training loss = 0.009281047775099674, validation loss = 0.009169611148536205\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 44, training loss = 0.00921618208910028, validation loss = 0.009191715158522129\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 45, training loss = 0.0092541608804216, validation loss = 0.00917532667517662\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 46, training loss = 0.009238284572751986, validation loss = 0.009175860323011875\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 47, training loss = 0.00923553018623756, validation loss = 0.009161500260233879\n",
      "epoch = 48, training loss = 0.009285460821249418, validation loss = 0.009167867712676525\n",
      "EarlyStopping counter: 1 out of 10\n",
      "epoch = 49, training loss = 0.009251052617198892, validation loss = 0.009162046015262604\n",
      "EarlyStopping counter: 2 out of 10\n",
      "epoch = 50, training loss = 0.009266909987976154, validation loss = 0.009161978960037231\n",
      "EarlyStopping counter: 3 out of 10\n",
      "epoch = 51, training loss = 0.00924657872464094, validation loss = 0.00916533824056387\n",
      "EarlyStopping counter: 4 out of 10\n",
      "epoch = 52, training loss = 0.009224791907601886, validation loss = 0.00916675291955471\n",
      "EarlyStopping counter: 5 out of 10\n",
      "epoch = 53, training loss = 0.009250579143149985, validation loss = 0.009189609438180923\n",
      "EarlyStopping counter: 6 out of 10\n",
      "epoch = 54, training loss = 0.009288290049880743, validation loss = 0.009181453846395016\n",
      "EarlyStopping counter: 7 out of 10\n",
      "epoch = 55, training loss = 0.009252441503728429, validation loss = 0.009193653240799904\n",
      "EarlyStopping counter: 8 out of 10\n",
      "epoch = 56, training loss = 0.009260687459674146, validation loss = 0.009173803962767124\n",
      "EarlyStopping counter: 9 out of 10\n",
      "epoch = 57, training loss = 0.00928430300619867, validation loss = 0.009183123707771301\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early stopping\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import torch             # torch基础库\n",
    "import torch.nn as nn    # torch神经网络库\n",
    "from torch.utils.data import DataLoader, Dataset \n",
    "from early_stopping import EarlyStopping \n",
    "import os, gc \n",
    "import random \n",
    "import subprocess \n",
    "import rpy2.robjects as robjects \n",
    "\n",
    "\n",
    "\n",
    "def get_gpu_memory(device_id):\n",
    "    \"\"\"\n",
    "    Retrieve the memory usage of a specific GPU.\n",
    "\n",
    "    Parameters:\n",
    "    device_id: ID of the GPU device to query.\n",
    "\n",
    "    Returns: A tuple (memory_used, memory_total) in MB, or (None, None) if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--id={}\".format(device_id), \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"])\n",
    "        memory_used, memory_total = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\",\"))\n",
    "        return memory_used, memory_total\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def get_free_gpu():\n",
    "    \"\"\"\n",
    "    Find the GPU with the most available memory and return its device ID.\n",
    "    \n",
    "    Returns:\n",
    "    torch.device or None: The device with the most free memory if available, otherwise None.\n",
    "    \"\"\"\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    memory_usages = []\n",
    "    for device_id in device_ids:\n",
    "        memory_used, memory_total = get_gpu_memory(device_id)\n",
    "        if memory_used is not None and memory_total is not None:\n",
    "            memory_free = memory_total - memory_used\n",
    "            memory_usages.append((device_id, memory_free))\n",
    "        print(memory_total,memory_usages)\n",
    "    if len(memory_usages) > 0:\n",
    "        best_device_id = sorted(memory_usages, key=lambda x: x[1])[len(device_ids)-1][0]\n",
    "        device = torch.device(f\"cuda:{best_device_id}\")\n",
    "        return device\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class Args:\n",
    "    \"\"\"\n",
    "    A class to store tuning parameters for model training.\n",
    "    \n",
    "    Attributes:\n",
    "    batch_size: batch size.\n",
    "    lr: Learning rate for the optimizer.\n",
    "    nepoch: Total number of epochs for training.\n",
    "    patience: Number of epochs to wait for improvement before stopping early.\n",
    "    wide: Width of the model, representing the number of units in each layer.\n",
    "    depth: Depth of the model, representing the number of layers.\n",
    "    n_train (int): Sample size.\n",
    "    m_train (int): Sampling frequency.\n",
    "    biaoji (str): A unique identifier to aovid confusion.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size=10, lr =0.001, nepoch = 200, patience = 10, wide = 100, depth = 5, n_train=1, m_train=1) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.nepoch = nepoch \n",
    "        self.patience = patience \n",
    "        self.wide = wide \n",
    "        self.depth = depth \n",
    "        self.biaoji = \"wide\" + str(wide) + \"depth\" + str(depth) + \"n\" + str(n_train) + \"m\" + str(m_train)\n",
    "        self.n_train = n_train\n",
    "        self.m_train = m_train\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping utility to halt training when validation loss does not improve.\n",
    "    \n",
    "    Attributes:\n",
    "    save_path: Path where model checkpoints are saved.\n",
    "    patience: Number of epochs to wait for improvement before stopping.\n",
    "    verbose: If True, prints validation loss improvements.\n",
    "    delta: Minimum change in validation loss to qualify as an improvement.\n",
    "    counter: Tracks epochs without improvement.\n",
    "    best_score: Best score achieved on validation loss (initialized to None).\n",
    "    early_stop: Flag to indicate if training should be stopped.\n",
    "    val_loss_min: Tracks the minimum validation loss seen so far.\n",
    "    \"\"\"\n",
    "    def __init__(self, save_path, args, verbose=False, delta=0):\n",
    "        self.save_path = save_path \n",
    "        self.patience = args.patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "\n",
    "    def __call__(self, model, train_loss, valid_loss, args, seed):\n",
    "        \"\"\"\n",
    "        Check if validation loss has improved and update early stopping criteria.\n",
    "        \n",
    "        Parameters:\n",
    "        model (torch.nn.Module): The model being trained.\n",
    "        train_loss: Training loss of the current epoch.\n",
    "        valid_loss: Validation loss of the current epoch.\n",
    "        test_error: Test error of the current epoch.\n",
    "        args (Args): Arguments class containing model parameters and settings.\n",
    "        seed (int): Seed for the current training run, used for unique file naming.\n",
    "        \"\"\"\n",
    "        score = -valid_loss \n",
    "\n",
    "        if self.best_score is None: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(model, train_loss, valid_loss, args, seed) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}') \n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, train_loss, valid_loss,  args, seed)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model, train_loss, valid_loss,  args, seed):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...') \n",
    "        torch.save(model, os.path.join(self.save_path, 'best' + str(seed) + args.biaoji +'network.pth') )\t# 这里会存储迄今最优模型的参数 \n",
    "        torch.save(train_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'train_loss.pth')) \n",
    "        torch.save(valid_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'valid_loss.pth')) \n",
    "\n",
    "        self.val_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "class Dataset_repeatedmeasurement(Dataset): \n",
    "    \"\"\"\n",
    "    A custom dataset class for handling repeated measurement data.\n",
    "    \n",
    "    Attributes:\n",
    "    x: Input data of features.\n",
    "    y: Target labels corresponding to the input data.\n",
    "    \n",
    "    Methods:\n",
    "    __len__: Returns the total number of samples in the dataset.\n",
    "    __getitem__: Retrieves a single sample, returning a dictionary with 'x' and 'y' keys.\n",
    "    \"\"\"\n",
    "    def __init__(self, x, y) -> None: \n",
    "        \"\"\"\n",
    "        Initialize the dataset with input data and corresponding labels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "\n",
    "    def __len__(self) -> int: \n",
    "        \"\"\"\n",
    "        Return the number of samples in the dataset.\n",
    "        \"\"\"\n",
    "        return len(self.x) \n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        \"\"\"\n",
    "        Retrieve a sample from the dataset at the specified index.\n",
    "        \"\"\"\n",
    "        return self.x[index], self.y[index]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class happynet(nn.Module):\n",
    "    \"\"\"\n",
    "    A flexible neural network with a customizable depth.\n",
    "    \n",
    "    Parameters:\n",
    "    n_feature: Dimension of input.\n",
    "    n_hidden: Number of units in each hidden layer.\n",
    "    n_output: Number of output units.\n",
    "    n_layer: the number of layers (supports 3 to 10 layers). \n",
    "             (n_layer-1) hidden layers\n",
    "    \n",
    "    Methods:\n",
    "    forward(x): Forward pass through the network.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_feature, n_hidden, n_output, n_layer): \n",
    "        super().__init__()\n",
    "        if n_layer == 3: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )\n",
    "        elif n_layer == 2: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )    \n",
    "        elif n_layer == 4: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )\n",
    "        elif n_layer == 5: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 6: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 7: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 8: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 9: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 10: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        else: \n",
    "            print(\"Error! the depth is not in 3-10\")\n",
    "    \n",
    "    #定义前向运算\n",
    "    def forward(self, x):\n",
    "        k = self.net(x)\n",
    "        return k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GPUstrain2(x, y, x_valid, y_valid, args,seed,nocuda):  \n",
    "    \"\"\"\n",
    "    Train a neural network on GPU or CPU based on the given configuration, using early stopping.\n",
    "    \n",
    "    Parameters:\n",
    "    x: Training input data.\n",
    "    y: Training target data.\n",
    "    x_valid: Validation input data.\n",
    "    y_valid: Validation target data.\n",
    "    x_test: Test input data.\n",
    "    y_test: Test target data.\n",
    "    args: Arguments object containing hyperparameters.\n",
    "    seed: Seed and identifier.\n",
    "    nocuda (int): Flag to select device; options include specific GPU IDs, CPU, or auto-selection.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (trained network, list of training losses, list of validation losses, list of test errors)\n",
    "    \"\"\"\n",
    "\n",
    "    x_dim = 1 \n",
    "\n",
    "    if nocuda == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    if nocuda == 1:\n",
    "        device = torch.device(\"cuda:1\")\n",
    "    if nocuda == 100:\n",
    "        device = get_free_gpu()\n",
    "    if nocuda == -1:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    net = happynet(n_feature=x_dim, n_hidden=args.wide, n_output=1, n_layer=args.depth).to(device)\n",
    "    nepoch = args.nepoch\n",
    "    \n",
    "    optimizer=torch.optim.Adam(net.parameters(), lr=args.lr, betas=(0.90, 0.999), eps=1e-8, weight_decay=0., amsgrad=False,)  \n",
    "    loss_func=nn.MSELoss() \n",
    "    train_epochs_loss = [] \n",
    "    valid_epochs_loss = [] \n",
    "    x = x.reshape(-1,x_dim)\n",
    "    y = y.reshape(-1)\n",
    "\n",
    "    x=torch.from_numpy(x).float().to(device)\n",
    "    y=torch.from_numpy(y).float().to(device)\n",
    "    x_valid=torch.from_numpy(x_valid).float().to(device) \n",
    "    y_valid=torch.from_numpy(y_valid).float().to(device) \n",
    "    train_dataset = Dataset_repeatedmeasurement(x,y)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    save_path = \"./resultsv\" \n",
    "    early_stopping = EarlyStopping(save_path,args=args)\n",
    "\n",
    "    for epoch in range(nepoch): \n",
    "        net.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "        # =========================train=========================\n",
    "        for idx, traindata in enumerate(train_dataloader):\n",
    "            x_train, y_train = traindata\n",
    "            x_train=torch.Tensor(x_train).float().view(-1,1,x_dim)\n",
    "            y_train=torch.Tensor(y_train).float()\n",
    "            outputs=net(x_train) \n",
    "\n",
    "            loss=loss_func(outputs.view(-1),y_train.view(-1).float())\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            loss.backward()  \n",
    "            optimizer.step()  \n",
    "            \n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "        del outputs, loss\n",
    "\n",
    "        train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "\n",
    "        # =========================valid=========================\n",
    "        with torch.no_grad():\n",
    "            net.eval() \n",
    "\n",
    "            valid_predict=net(x_valid.view(-1,1,x_dim))\n",
    "            valid_y_pre=valid_predict.view(-1).detach()\n",
    "            valid_y_pre=torch.Tensor(valid_y_pre).float()\n",
    "            loss_valid=loss_func(valid_y_pre, y_valid.view(-1).float())\n",
    "            valid_epochs_loss.append(loss_valid.item())\n",
    "\n",
    "\n",
    "\n",
    "        print(\"epoch = {}, training loss = {}, validation loss = {}\".format(epoch, np.average(train_epoch_loss), loss_valid)) \n",
    "\n",
    "\n",
    "        if epoch > 10 or args.n_train*args.m_train < 200:\n",
    "            early_stopping(net, np.average(train_epoch_loss), loss_valid, args,seed)\n",
    "            if early_stopping.early_stop: \n",
    "                print(\"Early stopping\")\n",
    "                break  \n",
    "\n",
    "\n",
    "        del valid_predict, valid_y_pre, loss_valid \n",
    "    gc.collect()\n",
    "\n",
    "    return net, train_epochs_loss, valid_epochs_loss  \n",
    "\n",
    "\n",
    "\n",
    "robjects.r['load'](\"./data/datahao/data2008.Rdata\") \n",
    "nnn_vec = [math.ceil(len(robjects.r['data'][0]) * 0.8 )]\n",
    "mma = [50]\n",
    "res = np.zeros(shape=(len(nnn_vec), len(mma), 3))\n",
    "\n",
    "for nnnind in range(len(nnn_vec)):\n",
    "    nnn = nnn_vec[nnnind]\n",
    "    n_train = nnn\n",
    "    n_valid = min(math.ceil(nnn * 0.25), len(robjects.r['data'][0]) - math.ceil(len(robjects.r['data'][0]) * 0.8 ))\n",
    "    torch.manual_seed(123456) \n",
    "    np.random.seed(123456) \n",
    "    random.seed(123456) \n",
    "    torch.cuda.manual_seed_all(123456) \n",
    "    randind = list(range(len(robjects.r['data'][0])))\n",
    "    random.shuffle(randind)\n",
    "    train_ind = [randind[i] for i in range(0, math.ceil(len(robjects.r['data'][0]) * 0.8 ))]\n",
    "    valid_ind = [randind[i] for i in range(math.ceil(len(robjects.r['data'][0]) * 0.8 ), len(robjects.r['data'][0]))]\n",
    "    train_ind = [train_ind[i] for i in range(n_train)]\n",
    "    valid_ind = [valid_ind[i] for i in range(n_valid)]\n",
    "\n",
    "    for mind in range(len(mma)):\n",
    "        m = mma[mind]\n",
    "        m_train = m\n",
    "        m_valid = m\n",
    "        y_train = np.array([]) \n",
    "        pp_train = np.array([]) \n",
    "        for i in train_ind:\n",
    "            random.seed(123*i) \n",
    "            train_ind_m = list(range(50))\n",
    "            random.shuffle(train_ind_m)\n",
    "            for jj in range(m):\n",
    "                j = train_ind_m[jj]\n",
    "                y_train = np.append(y_train, robjects.r['data'][1][i][j])\n",
    "                pp_train = np.append(pp_train, robjects.r['data'][0][i][j])\n",
    "                y_train = y_train.reshape(-1,1)\n",
    "                pp_train = pp_train.reshape(-1,1)\n",
    "\n",
    "        y_valid = np.array([]) \n",
    "        pp_valid = np.array([]) \n",
    "        for i in valid_ind:\n",
    "            random.seed(123*i*6) \n",
    "            train_ind_m = list(range(50))\n",
    "            random.shuffle(train_ind_m)\n",
    "            for jj in range(m):\n",
    "                j = train_ind_m[jj]\n",
    "                y_valid = np.append(y_valid, robjects.r['data'][1][i][j])\n",
    "                pp_valid = np.append(pp_valid, robjects.r['data'][0][i][j])\n",
    "                y_valid = y_valid.reshape(-1,1)\n",
    "                pp_valid = pp_valid.reshape(-1,1)\n",
    "\n",
    "        if n_train*m_train < 128:\n",
    "            batch_size= min(n_train*m_train, 32)\n",
    "            lr = 0.0005\n",
    "        elif n_train*m_train < 1024:\n",
    "            batch_size= 64\n",
    "            lr = 0.0005\n",
    "        elif n_train*m_train < 4096:\n",
    "            batch_size= 128\n",
    "            lr = 0.001\n",
    "        elif n_train*m_train < 8192:\n",
    "            batch_size= 256\n",
    "            lr = 0.001\n",
    "        elif n_train*m_train < 16384:\n",
    "            batch_size= 512\n",
    "            lr = 0.002\n",
    "        elif n_train*m_train < 32768:\n",
    "            batch_size= 1024\n",
    "            lr = 0.003\n",
    "        elif n_train*m_train < 65536:\n",
    "            batch_size= 2048\n",
    "            lr = 0.005\n",
    "        elif n_train*m_train < 262144:\n",
    "            batch_size= 4096\n",
    "            lr = 0.008\n",
    "        else:\n",
    "            batch_size= 8192\n",
    "            lr = 0.01\n",
    "        nocuda = -1\n",
    "        torch.manual_seed(123) \n",
    "        np.random.seed(123) \n",
    "        random.seed(123) \n",
    "        torch.cuda.manual_seed_all(123) \n",
    "\n",
    "        args = Args(lr=lr, wide=50, depth = 2, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net0 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c0 = np.r_[a,b,0]\n",
    "\n",
    "        args = Args(lr=lr, wide=100, depth = 3, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net1 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c1 = np.r_[a,b,1]\n",
    "\n",
    "\n",
    "        args = Args(lr=lr, wide=200, depth = 4, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net2 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c2 = np.r_[a,b,2]\n",
    "\n",
    "        args = Args(lr=lr, wide=400, depth = 5, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net3 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c3 = np.r_[a,b,3]\n",
    "\n",
    "        args = Args(lr=lr, wide=600, depth = 6, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net4 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c4 = np.r_[a,b,4]\n",
    "        \n",
    "        args = Args(lr=lr, wide=800, depth = 6, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net5 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c5 = np.r_[a,b,5]\n",
    "\n",
    "        p = np.r_[np.expand_dims(c0, 0),np.expand_dims(c1, 0),np.expand_dims(c2, 0),np.expand_dims(c3, 0),np.expand_dims(c4, 0),np.expand_dims(c5, 0)]\n",
    "        ind = np.argmin(p[:,1])\n",
    "        res[nnnind,mind,:]=p[ind]\n",
    "        torch.save( eval(\"net\"+str(ind)) , os.path.join(\"./bestnet/bestfullnet.pth\") )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
