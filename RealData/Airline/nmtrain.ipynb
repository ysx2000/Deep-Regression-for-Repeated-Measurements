{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.path.append(\"./731vic/fun\")\n",
    "# import trainfun\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csaps\n",
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import torch             # torch基础库\n",
    "import torch.nn as nn    # torch神经网络库\n",
    "import torch.nn.functional as F    # torch神经网络库 \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image \n",
    "import imageio \n",
    "from early_stopping import EarlyStopping\n",
    "import os, gc\n",
    "import random \n",
    "import pynvml\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import subprocess\n",
    "import gc\n",
    "from functools import partial\n",
    "from pandarallel import pandarallel\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "\n",
    "\n",
    "def get_gpu_memory(device_id):\n",
    "    try:\n",
    "        output = subprocess.check_output([\"nvidia-smi\", \"--id={}\".format(device_id), \"--query-gpu=memory.used,memory.total\", \"--format=csv,nounits,noheader\"])\n",
    "        memory_used, memory_total = map(int, output.decode(\"utf-8\").strip().split(\"\\n\")[0].split(\",\"))\n",
    "        return memory_used, memory_total\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None, None\n",
    "\n",
    "def get_free_gpu():\n",
    "    device_ids = list(range(torch.cuda.device_count()))\n",
    "    memory_usages = []\n",
    "    for device_id in device_ids:\n",
    "        memory_used, memory_total = get_gpu_memory(device_id)\n",
    "        if memory_used is not None and memory_total is not None:\n",
    "            memory_free = memory_total - memory_used\n",
    "            memory_usages.append((device_id, memory_free))\n",
    "        print(memory_total,memory_usages)\n",
    "    if len(memory_usages) > 0:\n",
    "        best_device_id = sorted(memory_usages, key=lambda x: x[1])[len(device_ids)-1][0]\n",
    "        # print(sorted(memory_usages, key=lambda x: x[1]))\n",
    "        device = torch.device(f\"cuda:{best_device_id}\")\n",
    "        return device\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self, batch_size=10, lr =0.001, nepoch = 200, patience = 10, wide = 100, depth = 5, n_train=1, m_train=1) -> None:\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.nepoch = nepoch \n",
    "        self.patience = patience \n",
    "        self.wide = wide \n",
    "        self.depth = depth \n",
    "        self.biaoji = \"wide\" + str(wide) + \"depth\" + str(depth) + \"n\" + str(n_train) + \"m\" + str(m_train)\n",
    "        self.n_train = n_train\n",
    "        self.m_train = m_train\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, save_path, args, verbose=False, delta=0):\n",
    "        self.save_path = save_path \n",
    "        self.patience = args.patience \n",
    "        self.verbose = verbose \n",
    "        self.counter = 0 \n",
    "        self.best_score = None \n",
    "        self.early_stop = False \n",
    "        self.val_loss_min = np.Inf \n",
    "        self.delta = delta \n",
    "\n",
    "    def __call__(self, model, train_loss, valid_loss, args, seed):\n",
    "\n",
    "        score = -valid_loss \n",
    "\n",
    "        if self.best_score is None: \n",
    "            self.best_score = score \n",
    "            self.save_checkpoint(model, train_loss, valid_loss, args, seed) \n",
    "        elif score < self.best_score + self.delta: \n",
    "            self.counter += 1 \n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}') \n",
    "            if self.counter >= self.patience: \n",
    "                self.early_stop = True \n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(model, train_loss, valid_loss,  args, seed)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, model, train_loss, valid_loss,  args, seed):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {valid_loss:.6f}).  Saving model ...') \n",
    "        # path = os.path.join(self.save_path, 'best'+ args.biaoji +'network.pth') \n",
    "        torch.save(model, os.path.join(self.save_path, 'best' + str(seed) + args.biaoji +'network.pth') )\t# 这里会存储迄今最优模型的参数 \n",
    "        torch.save(train_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'train_loss.pth')) \n",
    "        torch.save(valid_loss, os.path.join(self.save_path, 'best'+ str(seed) + args.biaoji +'valid_loss.pth')) \n",
    "\n",
    "        self.val_loss_min = valid_loss\n",
    "    \n",
    "\n",
    "\n",
    "class Dataset_repeatedmeasurement(Dataset): \n",
    "    def __init__(self, x, y) -> None:  #括号里是要赋予的\n",
    "        super().__init__()\n",
    "    #    assert np.allclose([x.shape[0],x.shape[1]],[y.shape[0],y.shape[1]]), \"Error! x and y have different shape\"\n",
    "        self.x = x \n",
    "        self.y = y \n",
    "        # self.nsubject = x.shape[0]\n",
    "        # self.mobserv = x.shape[1]\n",
    "        # self.dimension = x.shape[2]\n",
    "\n",
    "    def __len__(self) -> int: \n",
    "    #    aaa = self.x.shape[0]*self.x.shape[1] \n",
    "    #    aaa = self.x.shape[0]\n",
    "        return len(self.x) \n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        # a = self.x.shape[0] \n",
    "        # b = self.x.shape[1] \n",
    "        # a2 = index // b \n",
    "        # b2 = index % b \n",
    "        # return {\n",
    "        #     \"x\" : self.x[a2][b2], \n",
    "        #     \"y\" : self.y[a2][b2]\n",
    "        # }\n",
    "        return {\n",
    "            \"x\" : self.x[index], \n",
    "            \"y\" : self.y[index]\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class happynet(nn.Module):\n",
    "    def __init__(self, trun,  n_feature, n_hidden, n_output, n_layer): \n",
    "        super().__init__()\n",
    "        self.trun = trun \n",
    "        if n_layer == 3: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )\n",
    "        elif n_layer == 2: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )    \n",
    "        elif n_layer == 4: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output), \n",
    "            )\n",
    "        elif n_layer == 5: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 6: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 7: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 8: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 9: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        elif n_layer == 10: \n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(n_feature, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),  \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden), \n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(n_hidden, n_output),\n",
    "            )\n",
    "        else: \n",
    "            print(\"Error! the depth is not in 3-10\")\n",
    "    \n",
    "    #定义前向运算\n",
    "    def forward(self, x):\n",
    "        k = self.net(x)\n",
    "        # if k[0,0,0] > self.trun:\n",
    "        #     k[0,0,0] = self.trun\n",
    "        # if k[0,0,0] < -self.trun:\n",
    "        #     k[0,0,0] = -self.trun\n",
    "        return k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GPUstrain2(x, y, x_valid, y_valid, args,seed,nocuda,trun): # batch_size, nepoch=200): \n",
    "\n",
    "    x_dim = 1\n",
    "    # device = get_free_gpu()\n",
    "    # device = torch.device(\"cpu\")\n",
    "    #device = torch.device(\"cuda:1\")\n",
    "\n",
    "    if nocuda == 0:\n",
    "        device = torch.device(\"cuda:0\")\n",
    "    if nocuda == 1:\n",
    "        device = torch.device(\"cuda:1\")\n",
    "    if nocuda == 100:\n",
    "        device = get_free_gpu()\n",
    "    if nocuda == -1:\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "\n",
    "    net = happynet(trun=trun, n_feature=x_dim, n_hidden=args.wide, n_output=1, n_layer=args.depth).to(device)\n",
    "    nepoch = args.nepoch\n",
    "    \n",
    "    optimizer=torch.optim.Adam(net.parameters(), lr=args.lr, betas=(0.90, 0.999), eps=1e-8, weight_decay=0., amsgrad=False,) \n",
    "    # lr(Learning Rate) \n",
    "    # betas (Tuple[float, float], 可选) – 用于计算梯度以及梯度平方的运行平均值的系数（默认：0.9，0.999）\n",
    "    # eps (float, 可选) – 为了增加数值计算的稳定性而加到分母里的项（默认：1e-8）\n",
    "    # weight_decay (float, 可选) – 权重衰减（L2惩罚）（默认: 0）\n",
    "    loss_func=nn.MSELoss() \n",
    "    train_epochs_loss = [] \n",
    "    valid_epochs_loss = [] \n",
    "    x = x.reshape(-1,x_dim)\n",
    "    y = y.reshape(-1)\n",
    "\n",
    "    train_dataset = Dataset_repeatedmeasurement(x,y)\n",
    "    train_dataloader = DataLoader(dataset=train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "    x=torch.from_numpy(x).float() \n",
    "    y=torch.from_numpy(y).float() \n",
    "    x_valid=torch.from_numpy(x_valid).float().to(device) \n",
    "    y_valid=torch.from_numpy(y_valid).float().to(device) \n",
    "\n",
    "\n",
    "\n",
    "    save_path = \"./resultsv\" \n",
    "    early_stopping = EarlyStopping(save_path,args=args)\n",
    "\n",
    "    for epoch in range(nepoch): \n",
    "        net.train()\n",
    "        train_epoch_loss = []\n",
    "\n",
    "\n",
    "\n",
    "        # =========================train=========================\n",
    "        for idx, traindata in enumerate(train_dataloader):\n",
    "\n",
    "            x_train = traindata[\"x\"]\n",
    "            y_train = traindata[\"y\"]\n",
    "\n",
    "            x_train=torch.Tensor(x_train).float().view(-1,1,x_dim).to(device) \n",
    "            y_train=torch.Tensor(y_train).float().to(device) \n",
    "\n",
    "            outputs=net(x_train) \n",
    "\n",
    "            loss=loss_func(outputs.view(-1),y_train.view(-1).float())\n",
    "\n",
    "            optimizer.zero_grad() # 把梯度置零，也就是把loss关于weight的导数变成0，即将梯度初始化为零\n",
    "\n",
    "            loss.backward() # 对loss进行反向传播\n",
    "            optimizer.step() # 再对梯度进行优化，更新所有参数\n",
    "            \n",
    "            \n",
    "            train_epoch_loss.append(loss.item())\n",
    "\n",
    "        del outputs, loss\n",
    "\n",
    "        train_epochs_loss.append(np.average(train_epoch_loss))\n",
    "\n",
    "\n",
    "        # print(\" loss = {}\".format(np.average(train_epoch_loss)))\n",
    "        # =========================valid=========================\n",
    "        with torch.no_grad():\n",
    "            net.eval() \n",
    "\n",
    "            valid_predict=net(x_valid.view(-1,1,x_dim))\n",
    "            valid_y_pre=valid_predict.view(-1).detach()\n",
    "            valid_y_pre=torch.Tensor(valid_y_pre).float()\n",
    "            loss_valid=loss_func(valid_y_pre, y_valid.view(-1).float())\n",
    "            valid_epochs_loss.append(loss_valid.item())\n",
    "\n",
    "\n",
    "\n",
    "        print(\"epoch = {}, training loss = {}, validation loss = {}\".format(epoch, np.average(train_epoch_loss), loss_valid))#, error_test\n",
    "\n",
    "\n",
    "        if epoch > 10 or args.n_train*args.m_train > 200:\n",
    "            early_stopping(net, np.average(train_epoch_loss), loss_valid, args,seed)\n",
    "            if early_stopping.early_stop: \n",
    "                print(\"Early stopping\")\n",
    "                break #跳出迭代，结束训练\n",
    "\n",
    "\n",
    "        del valid_predict, valid_y_pre, loss_valid\n",
    "        # del test_predict, test_y_pre, error_test\n",
    "    gc.collect()\n",
    "\n",
    "    return net, train_epochs_loss, valid_epochs_loss # , test_epochs_error\n",
    "\n",
    "\n",
    "torch.manual_seed(12345) \n",
    "np.random.seed(12345) \n",
    "random.seed(12345) \n",
    "torch.cuda.manual_seed_all(12345) \n",
    "\n",
    "robjects.r['load'](\"./data/datahao/data2008.Rdata\") \n",
    "\n",
    "nnn_vec = [100,500,1000,5000]\n",
    "mma = [1,5,10,20,30,25,50]\n",
    "res = np.zeros(shape=(len(nnn_vec), len(mma), 3))\n",
    "\n",
    "for nnnind in range(len(nnn_vec)):\n",
    "    nnn = nnn_vec[nnnind]\n",
    "    n_train = nnn\n",
    "    n_valid = math.ceil(nnn * 0.25)\n",
    "    torch.manual_seed(123456) \n",
    "    np.random.seed(123456) \n",
    "    random.seed(123456) \n",
    "    torch.cuda.manual_seed_all(123456) \n",
    "    randind = list(range(len(robjects.r['data'][0])))\n",
    "    random.shuffle(randind)\n",
    "    train_ind = [randind[i] for i in range(0, math.ceil(len(robjects.r['data'][0]) * 0.8 ))]\n",
    "    valid_ind = [randind[i] for i in range(math.ceil(len(robjects.r['data'][0]) * 0.8 ), len(robjects.r['data'][0]))]\n",
    "    train_ind = [train_ind[i] for i in range(n_train)]\n",
    "    valid_ind = [valid_ind[i] for i in range(n_valid)]\n",
    "\n",
    "    for mind in range(len(mma)):\n",
    "        m = mma[mind]\n",
    "        m_train = m\n",
    "        m_valid = m\n",
    "        y_train = np.array([]) \n",
    "        pp_train = np.array([]) \n",
    "        for i in train_ind:\n",
    "            random.seed(123*i) \n",
    "            train_ind_m = list(range(50))\n",
    "            random.shuffle(train_ind_m)\n",
    "            # print(len(datav[i][0]))\n",
    "            for jj in range(m):\n",
    "                j = train_ind_m[jj]\n",
    "                y_train = np.append(y_train, robjects.r['data'][1][i][j])\n",
    "                pp_train = np.append(pp_train, robjects.r['data'][0][i][j])\n",
    "                y_train = y_train.reshape(-1,1)\n",
    "                pp_train = pp_train.reshape(-1,1)\n",
    "\n",
    "        y_valid = np.array([]) \n",
    "        pp_valid = np.array([]) \n",
    "        for i in valid_ind:\n",
    "            random.seed(123*i*6) \n",
    "            train_ind_m = list(range(50))\n",
    "            random.shuffle(train_ind_m)\n",
    "            # print(len(datav[i][0]))\n",
    "            for jj in range(m):\n",
    "                j = train_ind_m[jj]\n",
    "                y_valid = np.append(y_valid, robjects.r['data'][1][i][j])\n",
    "                pp_valid = np.append(pp_valid, robjects.r['data'][0][i][j])\n",
    "                y_valid = y_valid.reshape(-1,1)\n",
    "                pp_valid = pp_valid.reshape(-1,1)\n",
    "\n",
    "        if n_train*m_train < 128:\n",
    "            batch_size= min(n_train*m_train, 32)\n",
    "            lr = 0.0005\n",
    "        elif n_train*m_train < 1024:\n",
    "            batch_size= 64\n",
    "            lr = 0.0005\n",
    "        elif n_train*m_train < 4096:\n",
    "            batch_size= 128\n",
    "            lr = 0.001\n",
    "        elif n_train*m_train < 8192:\n",
    "            batch_size= 256\n",
    "            lr = 0.001\n",
    "        elif n_train*m_train < 16384:\n",
    "            batch_size= 512\n",
    "            lr = 0.002\n",
    "        else:\n",
    "            batch_size= 1024\n",
    "            lr = 0.002\n",
    "        nocuda = 0\n",
    "        trun = 30\n",
    "        torch.manual_seed(123) \n",
    "        np.random.seed(123) \n",
    "        random.seed(123) \n",
    "        torch.cuda.manual_seed_all(123) \n",
    "\n",
    "        args = Args(lr=lr, wide=50, depth = 2, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net0 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c0 = np.r_[a,b,0]\n",
    "\n",
    "        args = Args(lr=lr, wide=100, depth = 3, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net1 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c1 = np.r_[a,b,1]\n",
    "\n",
    "\n",
    "        args = Args(lr=lr, wide=200, depth = 4, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net2 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c2 = np.r_[a,b,2]\n",
    "\n",
    "        args = Args(lr=lr, wide=400, depth = 5, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net3 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c3 = np.r_[a,b,3]\n",
    "\n",
    "        args = Args(lr=lr, wide=600, depth = 6, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net4 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c4 = np.r_[a,b,4]\n",
    "        \n",
    "        args = Args(lr=lr, wide=800, depth = 6, batch_size= batch_size, n_train=n_train, m_train=m_train)\n",
    "        GPUstrain2(x=pp_train,y=y_train,x_valid = pp_valid,y_valid=y_valid, args=args,seed = 123, nocuda = nocuda, trun=trun)\n",
    "        a = torch.load('./resultsv/best'+ str(123) + args.biaoji +'train_loss.pth')\n",
    "        b = torch.load('./resultsv/best'+ str(123) + args.biaoji +'valid_loss.pth')\n",
    "        net5 = torch.load('./resultsv/best'+ str(123) + args.biaoji +'network.pth')\n",
    "        a = np.expand_dims(a , 0)\n",
    "        b = np.expand_dims(b.cpu(), 0)\n",
    "        c5 = np.r_[a,b,5]\n",
    "\n",
    "        p = np.r_[np.expand_dims(c0, 0),np.expand_dims(c1, 0),np.expand_dims(c2, 0),np.expand_dims(c3, 0),np.expand_dims(c4, 0),np.expand_dims(c5, 0)]\n",
    "        ind = np.argmin(p[:,1])\n",
    "        res[nnnind,mind,:]=p[ind]\n",
    "        torch.save( eval(\"net\"+str(ind)) , os.path.join(\"./bestnet/\", 'best' + str(nnn) +\"m\"+ str(m) +'net.pth') )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
